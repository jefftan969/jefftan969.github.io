<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Distilling Neural Fields for Real-Time Articulated Shape Reconstruction</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main" style="max-width: 800px">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Distilling Neural Fields for Real-Time <br> Articulated Shape Reconstruction </br>
                <small>CVPR 2023</small>
            </h2>
            <center>
                <a href="https://jefftan969.github.io/">Jeff Tan</a>,
                <a href="https://gengshan-y.github.io/">Gengshan Yang</a>,
                <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
                <br>
                Carnegie Mellon University
            </center>
        </div>

        
        <div class="row">
            <div class="col-md-6 col-md-offset-3 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://jefftan969.github.io/dasr/paper.pdf">
                        <image src="img/dasr_paper_image.png" height="60px">
                        <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://jefftan969.github.io/dasr/poster.pdf">
                        <image src="img/dasr_poster_image.png" height="30px" style="margin-top:30px">
                        <h4><strong>Poster</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.youtube.com/watch?v=taUtXtW8b3Q">
                        <image src="img/youtube_icon.png" height="60px">
                        <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/jefftan969/dasr">
                        <image src="img/github_icon.png" height="60px">
                        <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-12">
                <h3>Abstract</h3>
                <p class="text-justify">
We present a method for reconstructing articulated 3D models from videos in real-time, without test-time optimization or manual 3D supervision at training time. Prior work often relies on pre-built deformable models (e.g. SMAL/SMPL), or slow per-scene optimization through differentiable rendering (e.g. dynamic NeRFs). Such methods fail to support arbitrary object categories, or are unsuitable for real-time applications. To address the challenge of collecting large-scale 3D training data for arbitrary deformable object categories, our key insight is to use off-the-shelf video-based dynamic NeRFs as 3D supervision to train a fast feed-forward network, turning 3D shape and motion prediction into a supervised distillation task. Our temporal-aware network uses articulated bones and blend skinning to represent arbitrary deformations, and is self-supervised on video datasets without requiring 3D shapes or viewpoints as input. Through distillation, our network learns to 3D-reconstruct unseen articulated objects at interactive frame rates. Our method yields higher-fidelity 3D reconstructions than prior real-time methods for animals, with the ability to render realistic images at novel viewpoints and poses.
                </p>

                <h3>Video Results on Dogs</h3>
                <p class="text-justify">
From left to right: (1) The input image, (2) Comparison to BANMo, (3) Comparison to BARC, (4) Our articulated shape and texture predictions, and (5-7) Our predicted geometry from three views
                </p>
                <video id="dog_dog-dualrig-fgbg000" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-dualrig-fgbg000.mp4" type="video/mp4" />
                </video>
                <video id="dog_dog-pexel-w011" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-pexel-w011.mp4" type="video/mp4" />
                </video>
                <video id="dog_dog-pexel-e011" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-pexel-e011.mp4" type="video/mp4" />
                </video>
                <video id="dog_dog-pexel-q011" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-pexel-q011.mp4" type="video/mp4" />
                </video>
                <video id="dog_dog-pexel-t008" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-pexel-t008.mp4" type="video/mp4" />
                </video>
                <video id="dog_dog-pexel-t009" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-pexel-t009.mp4" type="video/mp4" />
                </video>
                <video id="dog_dog-pexel-y010" width="100%" autoplay loop muted>
                  <source src="img/dog_results/dog-pexel-y010.mp4" type="video/mp4" />
                </video>
                <video id="dog_shiba-haru-1012" width="100%" autoplay loop muted>
                  <source src="img/dog_results/shiba-haru-1012.mp4" type="video/mp4" />
                </video>

                <h3>Video Results on Cats</h3>
                <p class="text-justify">
From left to right: (1) The input image, (2) Comparison to BANMo, (3) Our articulated shape and texture predictions, and (4-6) Our predicted geometry from three views
                </p>
                <video id="cat_catpika-dualrig-fgbg004" width="100%" autoplay loop muted>
                  <source src="img/cat_results/catpika-dualrig-fgbg004.mp4" type="video/mp4" />
                </video>
                <video id="cat_cat-coco004" width="100%" autoplay loop muted>
                  <source src="img/cat_results/cat-coco004.mp4" type="video/mp4" />
                </video>
                <video id="cat_cat-pexel20016" width="100%" autoplay loop muted>
                  <source src="img/cat_results/cat-pexel20016.mp4" type="video/mp4" />
                </video>
                <video id="cat_cat-pexel30024" width="100%" autoplay loop muted>
                  <source src="img/cat_results/cat-pexel30024.mp4" type="video/mp4" />
                </video>
                <video id="cat_pikachiu170035" width="100%" autoplay loop muted>
                  <source src="img/cat_results/pikachiu170035.mp4" type="video/mp4" />
                </video>

                <h3>Video Results on Humans</h3>
                <p class="text-justify">
From left to right: (1) The input image, (2) Comparison to BANMo, (3) Our articulated shape and texture predictions, and (4-6) Our predicted geometry from three views
                </p>
                <video id="human_T_samba1" width="100%" autoplay loop muted>
                  <source src="img/human_results/T_samba1.mp4" type="video/mp4" />
                </video>
                <video id="human_D_bouncing1" width="100%" autoplay loop muted>
                  <source src="img/human_results/D_bouncing1.mp4" type="video/mp4" />
                </video>
                <video id="human_D_handstand1" width="100%" autoplay loop muted>
                  <source src="img/human_results/D_handstand1.mp4" type="video/mp4" />
                </video>

                <h3>Bibtex</h3>
                <pre>
@inproceedings{tan2023distilling,
    title={Distilling Neural Fields for Real-Time Articulated Shape Reconstruction},
    author={Tan, Jeff and Yang, Gengshan and Ramanan, Deva},
    booktitle={CVPR},
    year={2023}
}</pre>

                <h3>Acknowledgments</h3>
                <p class="text-justify">
                <a href="https://gengshan-y.github.io/">Gengshan Yang</a> is supported by the Qualcomm Innovation Fellowship. Thanks to <a href="https://andrewsonga.github.io/">Chonghyuk Song</a> for providing data; <a href="https://www.linkedin.com/in/chittesh-thavamani">Chittesh Thavamani</a> for help with appearance prediction; and <a href="https://dunbar12138.github.io/">Kangle Deng</a>, <a href="https://linzhiqiu.github.io/">Zhiqiu Lin</a>, and <a href="https://ericaweng.github.io/">Erica Weng</a> for reviewing early drafts.
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
